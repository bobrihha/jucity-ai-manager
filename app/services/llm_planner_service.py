from __future__ import annotations

import json
import time
from dataclasses import dataclass
from typing import Any

import httpx

from app.config import settings


@dataclass(frozen=True)
class PlannerToolCall:
    name: str
    args: dict[str, Any]


@dataclass(frozen=True)
class PlannerOutput:
    intent: str
    mode: str
    reply: str
    questions: list[str]
    link: str | None
    tool_calls: list[PlannerToolCall]
    slot_updates: dict[str, Any]


@dataclass(frozen=True)
class PlannerResult:
    output: PlannerOutput
    provider: str
    model: str
    latency_ms: int


def _parse_strict_json(text: str) -> dict[str, Any]:
    text = text.strip()
    # tolerate code fences
    if text.startswith("```"):
        text = text.strip("`")
        # try to locate first { ... }
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise ValueError("Planner did not return JSON object")
    return json.loads(text[start : end + 1])


def _coerce_output(obj: dict[str, Any]) -> PlannerOutput:
    def _str(v: Any, field: str) -> str:
        if not isinstance(v, str) or not v.strip():
            raise ValueError(f"Planner field '{field}' must be non-empty string")
        return v.strip()

    intent = _str(obj.get("intent"), "intent")
    mode = _str(obj.get("mode"), "mode")
    reply = _str(obj.get("reply"), "reply")

    questions_raw = obj.get("questions") or []
    if not isinstance(questions_raw, list):
        raise ValueError("Planner field 'questions' must be list")
    questions = [str(q).strip() for q in questions_raw if str(q).strip()][:2]

    link = obj.get("link")
    if link is not None:
        link = str(link).strip() or None

    tool_calls_raw = obj.get("tool_calls") or []
    if not isinstance(tool_calls_raw, list):
        raise ValueError("Planner field 'tool_calls' must be list")
    tool_calls: list[PlannerToolCall] = []
    for item in tool_calls_raw[:2]:
        if not isinstance(item, dict):
            continue
        name = str(item.get("name") or "").strip()
        args = item.get("args") if isinstance(item.get("args"), dict) else {}
        if name:
            tool_calls.append(PlannerToolCall(name=name, args=dict(args)))

    slot_updates = obj.get("slot_updates") if isinstance(obj.get("slot_updates"), dict) else {}

    return PlannerOutput(
        intent=intent,
        mode=mode,
        reply=reply,
        questions=questions,
        link=link,
        tool_calls=tool_calls,
        slot_updates=dict(slot_updates),
    )


def _tool_schema() -> list[dict[str, Any]]:
    return [
        {"name": "tool_get_facts", "args": {"park_slug": "nn"}, "returns": {"facts": "object", "pages": "object"}},
        {"name": "tool_search_kb", "args": {"park_slug": "nn", "query": "..."}, "returns": {"chunks": "list"}},
        {
            "name": "tool_upsert_lead",
            "args": {"park_slug": "nn", "session_id": "uuid", "slot_updates": {"kids_count": 8}},
            "returns": {"lead_id": "uuid", "missing_slots": "list"},
        },
        {
            "name": "tool_create_handoff",
            "args": {"park_slug": "nn", "session_id": "uuid", "reason": "..."},
            "returns": {"handoff_created": True},
        },
    ]


def _planner_system_prompt() -> str:
    return (
        "–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞—Ä–∫–∞. –ù–µ –∑–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å '—á—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç'. –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –ø–æ –¥–µ–ª—É.\n"
        "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ –±–µ–∑ markdown.\n"
        "–§–æ—Ä–º–∞—Ç: { intent, mode, reply, questions, link, tool_calls, slot_updates }.\n"
        "questions: –º–∞—Å—Å–∏–≤ –∏–∑ 0..2 —Å—Ç—Ä–æ–∫. link: —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ null. tool_calls: –º–∞—Å—Å–∏–≤ 0..2 –æ–±—ä–µ–∫—Ç–æ–≤ {name,args}.\n"
        "slot_updates: –æ–±—ä–µ–∫—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º).\n"
        "–ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ ‚Äî –∑–∞–¥–∞–π 1 —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å (–≤ questions) –∏ –≤ reply –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n"
        "–°—Ç–∏–ª—å: –∫–æ—Ä–æ—Ç–∫–æ, –∂–∏–≤–æ, 1‚Äì2 —ç–º–æ–¥–∑–∏, –ª—ë–≥–∫–∏–π —é–º–æ—Ä —É–º–µ—Å—Ç–Ω–æ.\n"
        "–ù–µ–ª—å–∑—è: –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å —Ü–µ–Ω—ã/—Å—É–º–º—ã/—Ä—É–±–ª–∏, –¥–æ–±–∞–≤–ª—è—Ç—å —Å—Å—ã–ª–∫–∏ –∫—Ä–æ–º–µ link.\n"
    )


async def run_planner(
    *,
    user_message: str,
    channel: str,
    park_slug: str,
    session_id: str,
    user_id: str | None,
    tool_results: dict[str, Any] | None = None,
) -> PlannerResult:
    provider = settings.llm_planner_provider
    model = settings.llm_planner_model
    t0 = time.monotonic()

    if provider == "mock":
        data = _mock_planner(
            user_message=user_message,
            channel=channel,
            park_slug=park_slug,
            session_id=session_id,
            user_id=user_id,
            tool_results=tool_results or {},
        )
        latency_ms = int((time.monotonic() - t0) * 1000)
        return PlannerResult(output=_coerce_output(data), provider=provider, model="mock", latency_ms=latency_ms)

    if provider == "openai":
        if not settings.llm_planner_api_key:
            raise RuntimeError("LLM_PLANNER_API_KEY is required for openai planner")
        if not model:
            model = "gpt-4o-mini"

        payload = {
            "model": model,
            "input": [
                {"role": "system", "content": _planner_system_prompt()},
                {
                    "role": "user",
                    "content": json.dumps(
                        {
                            "park_slug": park_slug,
                            "channel": channel,
                            "session_id": session_id,
                            "user_id": user_id,
                            "user_message": user_message,
                            "tools": _tool_schema(),
                            "tool_results": tool_results or {},
                        },
                        ensure_ascii=False,
                    ),
                },
            ],
        }
        async with httpx.AsyncClient(timeout=25) as client:
            r = await client.post(
                "https://api.openai.com/v1/responses",
                headers={"Authorization": f"Bearer {settings.llm_planner_api_key}", "Content-Type": "application/json"},
                json=payload,
            )
            r.raise_for_status()
            resp = r.json()
        text = ""
        for item in resp.get("output", []):
            for c in item.get("content", []):
                if c.get("type") == "output_text" and c.get("text"):
                    text += c["text"]
        if not text:
            raise RuntimeError("Planner response had no text")
        obj = _parse_strict_json(text)
        latency_ms = int((time.monotonic() - t0) * 1000)
        return PlannerResult(output=_coerce_output(obj), provider=provider, model=model, latency_ms=latency_ms)

    raise RuntimeError(f"Unsupported LLM_PLANNER_PROVIDER: {provider}")


def _mock_planner(
    *,
    user_message: str,
    channel: str,
    park_slug: str,
    session_id: str,
    user_id: str | None,
    tool_results: dict[str, Any],
) -> dict[str, Any]:
    t = user_message.lower()
    have_facts = "tool_get_facts" in tool_results

    def call_get_facts() -> dict[str, Any]:
        return {
            "intent": "info",
            "mode": "consult_mode",
            "reply": "–°–µ–∫—É–Ω–¥—É, —É—Ç–æ—á–Ω—é –ø–æ –±–∞–∑–µ üëÄ",
            "questions": [],
            "link": None,
            "tool_calls": [{"name": "tool_get_facts", "args": {"park_slug": park_slug}}],
            "slot_updates": {},
        }

    if "/start" in t or t.strip() in {"/help", "/start"}:
        return {
            "intent": "start",
            "mode": "consult_mode",
            "reply": "–ü—Ä–∏–≤–µ—Ç! –Ø –î–∂—É—Å–∏ ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–∞—Ä–∫–∞ ¬´–î–∂—É–Ω–≥–ª–∏ –°–∏—Ç–∏¬ª üêíüå¥",
            "questions": ["–° —á–µ–≥–æ –Ω–∞—á–Ω—ë–º?"],
            "link": None,
            "tool_calls": [],
            "slot_updates": {},
        }

    if "—Å–∫—É—á–Ω" in t:
        return {
            "intent": "banter",
            "mode": "consult_mode",
            "reply": "–û–π, –ø—Ä–∏–Ω—è–ª–∞ üòÖ –î–∞–≤–∞–π —Å–¥–µ–ª–∞—é –ø–æ–ª–µ–∑–Ω–æ: —Å–ø—Ä–æ—Å–∏ –ø—Ä–æ –≥—Ä–∞—Ñ–∏–∫, –∫–∞–∫ –¥–æ–±—Ä–∞—Ç—å—Å—è –∏–ª–∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω ‚Äî –æ—Ç–≤–µ—á—É –ø–æ –¥–µ–ª—É.",
            "questions": [],
            "link": None,
            "tool_calls": [],
            "slot_updates": {},
        }

    if any(w in t for w in ["–ø–æ–µ—Å—Ç—å", "–ø–æ –∫—É—à–∞—Ç—å", "–ø–æ–∫—É—à–∞—Ç—å", "—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "–º–µ–Ω—é", "–µ–¥–∞", "–∫–∞—Ñ–µ"]):
        if not have_facts:
            return call_get_facts()
        pages = tool_results["tool_get_facts"].get("pages") or {}
        link = pages.get("restaurant") or None
        return {
            "intent": "restaurant",
            "mode": "consult_mode",
            "reply": "–î–∞, –≤ –ø–∞—Ä–∫–µ –µ—Å—Ç—å —Ä–µ—Å—Ç–æ—Ä–∞–Ω/–∫–∞—Ñ–µ üôÇ –ú–µ–Ω—é –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ ‚Äî –ø–æ —Å—Å—ã–ª–∫–µ.",
            "questions": ["–ù—É–∂–µ–Ω –ø–µ—Ä–µ–∫—É—Å –≤–æ –≤—Ä–µ–º—è –≤–∏–∑–∏—Ç–∞ –∏–ª–∏ –¥–ª—è –ø—Ä–∞–∑–¥–Ω–∏–∫–∞?"],
            "link": link,
            "tool_calls": [],
            "slot_updates": {},
        }

    if any(w in t for w in ["–¥—Ä", "–¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è", "–ø—Ä–∞–∑–¥–Ω–∏–∫", "–¥.—Ä"]):
        return {
            "intent": "party_main",
            "mode": "lead_mode",
            "reply": "–ö–ª–∞—Å—Å! –ü–æ–º–æ–≥—É —Å–æ–±—Ä–∞—Ç—å –∑–∞—è–≤–∫—É –Ω–∞ –ø—Ä–∞–∑–¥–Ω–∏–∫ üéÇ",
            "questions": ["–ù–∞ –∫–∞–∫—É—é –¥–∞—Ç—É –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ?", "–°–∫–æ–ª—å–∫–æ –¥–µ—Ç–µ–π –∏ –∫–∞–∫–æ–π –≤–æ–∑—Ä–∞—Å—Ç?"],
            "link": None,
            "tool_calls": [],
            "slot_updates": {},
        }

    return {
        "intent": "fallback",
        "mode": "consult_mode",
        "reply": "–ü–æ–Ω—è–ª–∞ üôÇ –°–ø—Ä–æ—Å–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ–µ ‚Äî –ø—Ä–æ –≥—Ä–∞—Ñ–∏–∫, –∫–∞–∫ –¥–æ–±—Ä–∞—Ç—å—Å—è, —Ü–µ–Ω—ã –∏–ª–∏ –ø—Ä–∞–∑–¥–Ω–∏–∫.",
        "questions": [],
        "link": None,
        "tool_calls": [],
        "slot_updates": {},
    }

